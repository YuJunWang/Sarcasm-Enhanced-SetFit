{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNMT7nQ6r5ZWh2sInwfITNY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. ç’°å¢ƒå®‰è£"],"metadata":{"id":"LgYlH-vPOCy1"}},{"cell_type":"markdown","source":["* è¨˜å¾—è¦å…ˆå°‡HuggingFace Api KEYè¨­å®šæ–¼Secrets\"HF_TOKEN\"\n","* è²·ä¸èµ·5090çš„å…è²»ä»”åªå¥½ç”¨colabç·´æ¨¡å‹"],"metadata":{"id":"UjJOnmdgOe4x"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMhmTAUJN5SV"},"outputs":[],"source":["# 1. å®‰è£ç’°å¢ƒ\n","!pip install -q datasets transformers torch accelerate setfit scikit-learn matplotlib seaborn\n","!pip install hf_xet\n","!pip install openpyxl\n","\n","# 2. æ›è¼‰ Google Drive\n","from google.colab import drive\n","import os\n","import shutil\n","\n","drive.mount('/content/drive')\n","\n","# è¨­å®šä½ çš„æª”æ¡ˆåœ¨ Drive ä¸­çš„è·¯å¾‘\n","DRIVE_PATH = \"/content/drive/MyDrive/Colab Notebooks/Project/Sarcasm-Enhanced-SetFit/imdb_analysis_results\"\n","os.makedirs(DRIVE_PATH, exist_ok=True) # è‡ªå‹•å»ºç«‹è³‡æ–™å¤¾\n","\n","# æª¢æŸ¥ä¸€ä¸‹æª”æ¡ˆåœ¨ä¸åœ¨\n","if os.path.exists(DRIVE_PATH):\n","    print(f\"âœ… æˆåŠŸé€£æ¥ Driveï¼Œæª”æ¡ˆåˆ—è¡¨: {os.listdir(DRIVE_PATH)}\")\n","else:\n","    print(\"âŒ æ‰¾ä¸åˆ°è·¯å¾‘ï¼Œè«‹ç¢ºèªä½ çš„è³‡æ–™å¤¾åç¨±æ˜¯å¦æ­£ç¢º\")\n","\n","\n","DRIVE_SAVE_PATH = \"/content/drive/MyDrive/Colab Notebooks/Project/Sarcasm-Enhanced-SetFit/Model_colab\"\n","os.makedirs(DRIVE_SAVE_PATH, exist_ok=True) # è‡ªå‹•å»ºç«‹è³‡æ–™å¤¾\n","\n","print(f\"ğŸ“‚ æ¨¡å‹å°‡å„²å­˜æ–¼é›²ç«¯è·¯å¾‘: {DRIVE_SAVE_PATH}\")"]},{"cell_type":"code","source":["# å„é …åŸºæœ¬è¨­å®š\n","USER_CONFIG = {\n","    \"data\": {\n","        \"dataset_name\": \"imdb\",\n","        \"split\": \"train\",\n","        \"seed_count\": 800,  # ç·´ç¿’ç”¨ç¨®å­è³‡æ–™æ•¸é‡\n","        \"random_seed\": 42\n","    },\n","    \"llm\": {\n","        \"model_id\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n","        \"temperature\": 0.1,  # é™ä½éš¨æ©Ÿæ€§ï¼Œä¿è­‰æ¨™è¨»ä¸€è‡´æ€§\n","        \"max_new_tokens\": 100\n","    },\n","    \"labels\": {\n","        \"aspects\": [\"Acting\", \"Plot/Story\", \"Visuals/Effects\", \"Pacing\"],\n","        \"sarcasm\": [\"Yes\", \"No\"]\n","    }\n","}"],"metadata":{"id":"P2uBoQEHPHWm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. æ ¸å¿ƒç¨‹å¼ç¢¼ï¼šGenAIæ¨™è¨»"],"metadata":{"id":"htuUt6MaO_06"}},{"cell_type":"markdown","source":["* é€™å€‹éƒ¨åˆ†æ˜¯åˆ©ç”¨ä¸‹è¼‰HFä¸Šçš„é–‹æºGenAIæ¨¡å‹(é€™é‚Šé¸çš„æ˜¯ **Qwen/Qwen2.5-1.5B-Instruct** )å°‡åˆå§‹è³‡æ–™å¿«é€Ÿåšåˆ†é¡ä»¥åŠåŠ ä¸Šåè«·æ¨™è¨»ï¼Œä½œç‚ºå¾ŒçºŒè¨“ç·´è³‡æ–™ã€‚"],"metadata":{"id":"O0_EDOPyP0fm"}},{"cell_type":"markdown","source":["## æ¨™è¨»è³‡æ–™ä¸»ç¨‹å¼"],"metadata":{"id":"8a8_3HeURdWl"}},{"cell_type":"code","source":["import torch\n","from datasets import load_dataset\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","import pandas as pd\n","from tqdm import tqdm\n","import json\n","import re\n","\n","# ==========================================\n","# 1. æ¨¡çµ„åŒ–å‡½å¼ (Modules)\n","# ==========================================\n","\n","def load_raw_data(config):\n","    \"\"\"è¼‰å…¥ IMDB è³‡æ–™é›†ä¸¦å–æ¨£\"\"\"\n","    print(f\"Loading {config['data']['dataset_name']} dataset...\")\n","    dataset = load_dataset(config['data']['dataset_name'], split=config['data']['split'])\n","\n","    # éš¨æ©Ÿæ‰“äº‚ä¸¦å–å‰ N ç­†\n","    sampled_data = dataset.shuffle(seed=config['data']['random_seed']).select(range(config['data']['seed_count']))\n","    print(f\"Data loaded. Sample size: {len(sampled_data)}\")\n","    return sampled_data\n","\n","def setup_genai_model(config):\n","    \"\"\"åˆå§‹åŒ– Qwen æ¨¡å‹èˆ‡ Pipeline\"\"\"\n","    print(f\"Loading LLM: {config['llm']['model_id']}...\")\n","\n","    tokenizer = AutoTokenizer.from_pretrained(config['llm']['model_id'])\n","    model = AutoModelForCausalLM.from_pretrained(\n","        config['llm']['model_id'],\n","        torch_dtype=torch.float16,\n","        device_map=\"auto\"\n","    )\n","\n","    # å»ºç«‹ Text Generation Pipeline\n","    pipe = pipeline(\n","        \"text-generation\",\n","        model=model,\n","        tokenizer=tokenizer,\n","        max_new_tokens=config['llm']['max_new_tokens'],\n","        temperature=config['llm']['temperature'],\n","        do_sample=True # Set to True for temperature to take effect, but keep temp low\n","    )\n","    return pipe\n","\n","def construct_prompt(review_text):\n","    \"\"\"\n","    å»ºæ§‹ Prompt å·¥ç¨‹ï¼š\n","    è¦æ±‚æ¨¡å‹æ‰®æ¼”å½±è©•åˆ†æå¸«ï¼Œä¸¦è¼¸å‡ºåš´æ ¼çš„ JSON æ ¼å¼ã€‚\n","    \"\"\"\n","\n","    system_prompt = f\"\"\"You are a strict film critic AI.\n","Your task is to classify the user's review into EXACTLY ONE category: [Acting, Visuals/Effects, Pacing, Plot/Story].\n","\n","**CRITICAL INSTRUCTION**: You must follow this priority order to decide:\n","\n","1. **PRIORITY 1 - Acting**: Does the review mention specific actor names, performance quality (stiff, amazing), or character believability?\n","   -> If YES, label as \"Acting\". (STOP HERE)\n","\n","2. **PRIORITY 2 - Visuals/Effects**: Does the review discuss CGI, lighting, camera work, atmosphere, or cinematography?\n","   -> If YES, label as \"Visuals/Effects\". (STOP HERE)\n","\n","3. **PRIORITY 3 - Pacing**: Does the review explicit complain about the movie's speed (too slow/fast) or length?\n","   -> If YES, label as \"Pacing\". (STOP HERE)\n","\n","4. **FALLBACK - Plot/Story**: Only if NONE of the above apply, and the review discusses the script, narrative, or general feelings.\n","   -> Label as \"Plot/Story\".\n","\n","Output strictly in JSON format:\n","{{\"aspect\": \"Acting\", \"sarcasm\": \"No\"}}\n","\"\"\"\n","\n","    user_prompt = f\"Review: \\\"{review_text[:1000]}\\\"...\" # æˆªæ–·éé•·è©•è«–ä»¥ç¯€çœè³‡æº\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\"role\": \"user\", \"content\": user_prompt}\n","    ]\n","    return messages\n","\n","def parse_llm_output(output_text):\n","    \"\"\"è§£æ LLM è¼¸å‡ºçš„ JSON\"\"\"\n","    try:\n","        # å˜—è©¦ç”¨ Regex æŠ“å– JSONå€å¡Šï¼Œé¿å…æ¨¡å‹è¼¸å‡ºé¡å¤–å»¢è©±\n","        match = re.search(r'\\{.*\\}', output_text, re.DOTALL)\n","        if match:\n","            json_str = match.group(0)\n","            data = json.loads(json_str)\n","            return data.get(\"aspect\", \"Unknown\"), data.get(\"sarcasm\", \"No\")\n","    except:\n","        pass\n","    return \"Unknown\", \"No\" # Fallback\n","\n","def run_auto_labeling(dataset, llm_pipe):\n","    \"\"\"åŸ·è¡Œæ‰¹æ¬¡æ¨™è¨»\"\"\"\n","    results = []\n","    print(\"Starting GenAI Auto-Annotation...\")\n","\n","    for item in tqdm(dataset):\n","        messages = construct_prompt(item['text'])\n","\n","        # å¥—ç”¨ Chat Template\n","        prompt = llm_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","        # ç”Ÿæˆ\n","        outputs = llm_pipe(prompt)\n","        generated_text = outputs[0]['generated_text']\n","\n","        # æå–æ¨¡å‹å›ç­”çš„éƒ¨åˆ† (å»æ‰ Prompt)\n","        response = generated_text.split(\"<|im_start|>assistant\\n\")[-1].strip()\n","\n","        # è§£æ\n","        aspect, sarcasm = parse_llm_output(response)\n","\n","        results.append({\n","            \"text\": item['text'],\n","            \"label_original\": item['label'], # IMDB åŸæœ¬çš„ sentiment (0/1)\n","            \"aspect\": aspect,\n","            \"sarcasm\": sarcasm\n","        })\n","\n","    return pd.DataFrame(results)\n","\n","# ==========================================\n","# 2. ä¸»åŸ·è¡Œå€å¡Š (Main Execution)\n","# ==========================================\n","\n","# A. è¼‰å…¥è³‡æ–™\n","sampled_dataset = load_raw_data(USER_CONFIG)\n","\n","# B. è¨­å®šæ¨¡å‹\n","llm_pipeline = setup_genai_model(USER_CONFIG)\n","\n","# C. åŸ·è¡Œè‡ªå‹•æ¨™è¨»\n","labeled_df = run_auto_labeling(sampled_dataset, llm_pipeline)\n","\n","# D. æª¢æŸ¥çµæœ\n","print(\"\\n=== Annotation Sample ===\")\n","print(labeled_df[['aspect', 'sarcasm']].value_counts())\n","display(labeled_df.head(5))"],"metadata":{"id":"lylTNUP5PCCX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## å„²å­˜æ¨™è¨»å®Œæˆè³‡æ–™"],"metadata":{"id":"zJFD3QluRleA"}},{"cell_type":"code","source":["# 1. ç¢ºèªä½ çš„è¼¸å‡ºè³‡æ–™å¤¾å­˜åœ¨\n","OUTPUT_DIR = DRIVE_PATH\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","# 2. æ¨™è¨»è³‡æ–™åœ¨ 'labeled_df' é€™å€‹è®Šæ•¸è£¡\n","if 'labeled_df' in locals():\n","    print(f\"æ‰¾åˆ°æ¨™è¨»è³‡æ–™ï¼Œå…± {len(labeled_df)} ç­†ã€‚æ­£åœ¨å­˜æª”...\")\n","\n","    # --- å„²å­˜ç‚º CSV (æ¨è–¦çµ¦ä¹‹å¾Œè¨“ç·´æ¨¡å‹è®€å–) ---\n","    csv_path = os.path.join(OUTPUT_DIR, \"qwen_labeled_data.csv\")\n","    labeled_df.to_csv(csv_path, index=False, encoding='utf-8-sig') # sig å¯ä»¥é¿å… Excel æ‰“é–‹äº‚ç¢¼\n","    print(f\"âœ… CSV å·²å„²å­˜: {os.path.abspath(csv_path)}\")\n","\n","    # --- å„²å­˜ç‚º Excel (æ¨è–¦ç”¨ä¾†äººå·¥æª¢æŸ¥/ä¿®æ­£æ¨™ç±¤) ---\n","    excel_path = os.path.join(OUTPUT_DIR, \"qwen_labeled_data.xlsx\")\n","    labeled_df.to_excel(excel_path, index=False)\n","    print(f\"âœ… Excel å·²å„²å­˜: {os.path.abspath(excel_path)}\")\n","\n","else:\n","    print(\"âŒ éŒ¯èª¤ï¼šåœ¨è¨˜æ†¶é«”ä¸­æ‰¾ä¸åˆ° 'labeled_df' é€™å€‹è®Šæ•¸ï¼\")\n","    print(\"è«‹ç¢ºèªä½ å·²ç¶“è·‘å®Œ QWEN çš„æ¨™è¨»å€å¡Šï¼Œæˆ–è€…æª¢æŸ¥è®Šæ•¸åç¨±æ˜¯å¦ä¸åŒã€‚\")"],"metadata":{"id":"xd-O9iAkQ3Q7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## æª¢æŸ¥æ¨™è¨»è³‡æ–™"],"metadata":{"id":"jRbPgwusRpKN"}},{"cell_type":"code","source":["def check_per_category(df, samples_per_class=3):\n","    categories = df['aspect'].unique()\n","\n","    for cat in categories:\n","        print(f\"\\nğŸ”µ Checking Category: [{cat}]\")\n","        print(\"=\"*60)\n","\n","        # è©²åˆ†é¡ä¸­éš¨æ©ŸæŠ½ N ç­†ï¼Œå¦‚æœä¸å¤  N ç­†å°±å…¨æŠ“\n","        subset = df[df['aspect'] == cat]\n","        n = min(len(subset), samples_per_class)\n","\n","        if n == 0:\n","            print(\"(No data found for this category)\")\n","            continue\n","\n","        sample_rows = subset.sample(n)\n","\n","        for idx, row in sample_rows.iterrows():\n","            print(f\"ğŸ”´ Sarcasm: {row['sarcasm']}\")\n","            print(f\"ğŸ“ Review: {row['text'][:400]}...\") # é¡¯ç¤ºå‰ 400 å­—\n","            print(\"-\" * 30)\n","\n","# åŸ·è¡Œæª¢æŸ¥\n","check_per_category(labeled_df)"],"metadata":{"id":"r-NkLEUERslr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. SetFitè©•è«–é¡å‹åˆ†é¡æ¨¡å‹è¨“ç·´"],"metadata":{"id":"7iLfiUGXR2k8"}},{"cell_type":"code","source":["import gc\n","import torch\n","\n","print(\"Cleaning up GPU memory...\")\n","\n","# 1. åˆªé™¤ LLM Pipeline ç‰©ä»¶ (åˆ‡æ–·å°æ¨¡å‹çš„å¼•ç”¨)\n","try:\n","    del llm_pipeline\n","except NameError:\n","    pass\n","\n","# 2. å¼·åˆ¶åŸ·è¡Œ Python åƒåœ¾å›æ”¶\n","gc.collect()\n","\n","# 3. æ¸…ç©º PyTorch çš„ GPU å¿«å– (é€™æ˜¯æœ€é—œéµçš„ä¸€æ­¥)\n","torch.cuda.empty_cache()\n","\n","print(\"GPU Memory Cleared! Ready for training.\")\n","\n","# æª¢æŸ¥ç¾åœ¨å‰©é¤˜å¤šå°‘è¨˜æ†¶é«” (é¸ç”¨)\n","# print(torch.cuda.memory_summary(abbreviated=True))"],"metadata":{"id":"_6WEPIUNVYOJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## A. æœªåŠ Sarcasmæ¨™è¨˜"],"metadata":{"id":"EpR_VSRESKL5"}},{"cell_type":"code","source":["from setfit import SetFitModel, SetFitTrainer\n","from datasets import Dataset\n","from sklearn.model_selection import train_test_split\n","from sentence_transformers.losses import CosineSimilarityLoss\n","\n","# ==========================================\n","# æœªåŠ Sarcasmæ¨™è¨˜ : SetFit æ¨¡å‹è¨“ç·´\n","# ==========================================\n","\n","def prepare_dataset_for_setfit(df):\n","    \"\"\"\n","    è³‡æ–™å‰è™•ç†ï¼š\n","    SetFit éœ€è¦ standard çš„ HuggingFace Dataset æ ¼å¼ã€‚\n","    \"\"\"\n","    # 1. å°‡æ–‡å­—æ¨™ç±¤è½‰ç‚ºæ•¸å­— ID (Label Encoding)\n","    aspect_map = {label: i for i, label in enumerate(USER_CONFIG['labels']['aspects'])}\n","    # æ³¨æ„ï¼šå¦‚æœæœ‰æ¨¡å‹æ¨™è¨»å‡º \"Unknown\" æˆ–å…¶ä»–æ€ªæ±è¥¿ï¼Œæˆ‘å€‘è¦éæ¿¾æ‰æˆ–æ˜¯æ­¸é¡\n","    df_clean = df[df['aspect'].isin(USER_CONFIG['labels']['aspects'])].copy()\n","\n","    df_clean['label'] = df_clean['aspect'].map(aspect_map)\n","\n","    print(f\"Valid data for training: {len(df_clean)} / {len(df)}\")\n","    print(f\"Label Mapping: {aspect_map}\")\n","\n","    # 2. åˆ‡åˆ†è¨“ç·´é›†èˆ‡é©—è­‰é›†\n","    train_df, eval_df = train_test_split(df_clean, test_size=0.2, random_state=42)\n","\n","    # 3. è½‰ç‚º HF Dataset\n","    train_ds = Dataset.from_pandas(train_df)\n","    eval_ds = Dataset.from_pandas(eval_df)\n","\n","    return train_ds, eval_ds, aspect_map\n","\n","\n","def train_efficient_setfit(full_train_dataset, eval_dataset, label_mapping):\n","    \"\"\"\n","    è³‡æ–™ä¸­å››å€‹ç¨®é¡åˆ†å¸ƒä¸å¹³å‡ï¼Œä»¥Pacingæœ€å°‘ã€Plotæœ€å¤šåˆæœ€é›œã€‚\n","    æ ¹æ“šé€™å€‹åŸå‰‡è¨­å®šä¸åŒæ•¸é‡ä¸Ÿå…¥SetFitè¨“ç·´\n","    \"\"\"\n","    # 1. è½‰å› Pandas åšç²¾æº–æŠ½æ¨£\n","    df_train = full_train_dataset.to_pandas()\n","\n","    SAMPLE_QUOTAS = {\n","        0: 100,  # Acting: çµ¦ 100 ç­†\n","        1: 180,  # Plot: çµ¦ 180 ç­†\n","        2: 50,   # Visuals: 50\n","        3: 50    # Pacing: 50\n","    }\n","\n","    def dynamic_sample(group):\n","        label_id = group.name # å–å¾—ç›®å‰çš„ label ID\n","        # å–å¾—è©²é¡åˆ¥çš„é…é¡ï¼Œå¦‚æœæ²’è¨­å®šå°±é è¨­ 50\n","        quota = SAMPLE_QUOTAS.get(label_id, 50)\n","        # å¯¦éš›æŠ½æ¨£ï¼šå– (é…é¡) èˆ‡ (è©²é¡åˆ¥ç¸½æ•¸) çš„æœ€å°å€¼\n","        return group.sample(min(len(group), quota), random_state=42)\n","\n","\n","    # åŸ·è¡ŒæŠ½æ¨£\n","    df_sampled = df_train.groupby('label', group_keys=False).apply(dynamic_sample)\n","\n","    # å¼·åˆ¶é‡ç½® Index\n","    df_sampled = df_sampled.reset_index(drop=True)\n","\n","    # è½‰å› Dataset\n","    train_subset = Dataset.from_pandas(df_sampled, preserve_index=False)\n","\n","    print(f\"åŸæœ¬è¨“ç·´é›†: {len(full_train_dataset)} -> ğŸ“‰ å¹³è¡¡æŠ½æ¨£å¾Œ: {len(train_subset)}\")\n","    print(f\"æŠ½æ¨£å¾Œçš„é¡åˆ¥åˆ†å¸ƒ:\\n{df_sampled['label'].value_counts()}\")\n","\n","    # 2. è¼‰å…¥æ¨¡å‹\n","    model_id = \"sentence-transformers/paraphrase-mpnet-base-v2\"\n","    model = SetFitModel.from_pretrained(\n","        model_id,\n","        id2label={v: k for k, v in label_mapping.items()},\n","        label2id=label_mapping,\n","    )\n","\n","    # 3. è¨­å®šè¨“ç·´åƒæ•¸\n","    trainer = SetFitTrainer(\n","        model=model,\n","        train_dataset=train_subset, # é¤µå…¥æŠ½æ¨£å¾Œçš„è³‡æ–™\n","        eval_dataset=eval_dataset,\n","        loss_class=CosineSimilarityLoss,\n","        metric=\"accuracy\",\n","\n","        # --- æ•ˆç‡åƒæ•¸å€ ---\n","        batch_size=8,\n","        num_iterations=12,\n","        num_epochs=1,\n","        column_mapping={\"text\": \"text\", \"label\": \"label\"}\n","    )\n","\n","    # 4. é–‹å§‹è¨“ç·´\n","    print(\"ğŸš€ é–‹å§‹é«˜æ•ˆç‡è¨“ç·´ (Efficient Training)...\")\n","    trainer.train()\n","\n","    # 5. è©•ä¼°\n","    metrics = trainer.evaluate()\n","    print(f\"\\nğŸ“Š è¨“ç·´å®Œæˆã€‚Metrics: {metrics}\")\n","\n","    return model\n","\n","# ==========================================\n","# åŸ·è¡Œå€å¡Š\n","# ==========================================\n","\n","if 'labeled_df' in locals():\n","\n","    print(\"Found labeled_df, preparing datasets...\")\n","    # 1. æº–å‚™è³‡æ–™\n","    train_dataset, eval_dataset, label_mapping = prepare_dataset_for_setfit(labeled_df)\n","\n","    # 2. è¨“ç·´æ¨¡å‹\n","    model_a = train_efficient_setfit(train_dataset, eval_dataset, label_mapping)\n","\n","    # 3. ä¿å­˜æ¨¡å‹\n","    # é›²ç«¯\n","    save_path_a = os.path.join(DRIVE_SAVE_PATH, \"IMDB_aspect_model\")\n","    model_a.save_pretrained(save_path_a)\n","    print(f\"âœ… æ¨¡å‹æœ¬é«”å·²å„²å­˜: {save_path_a}\")\n","\n","else:\n","    raise ValueError(\"æ‰¾ä¸åˆ° labeled_dfï¼\")"],"metadata":{"id":"hr-eUfiVSC96"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## B. åŠ ä¸ŠSarcasmæ¨™è¨˜"],"metadata":{"id":"1KwF0j45TdDe"}},{"cell_type":"code","source":["from setfit import SetFitModel, SetFitTrainer\n","from datasets import Dataset\n","from sklearn.model_selection import train_test_split\n","from sentence_transformers.losses import CosineSimilarityLoss\n","\n","# ==========================================\n","# åŠ ä¸ŠSarcasmæ¨™è¨˜ : SetFit æ¨¡å‹è¨“ç·´\n","# ==========================================\n","def prepare_dataset_for_model_b(df):\n","    \"\"\"\n","    Model B å°ˆç”¨ï¼šè³‡æ–™å‰è™•ç†\n","    å·®ç•°é»ï¼šå°‡ Sarcasm è³‡è¨Šæ³¨å…¥åˆ° Text ä¸­ (Text Augmentation)\n","    \"\"\"\n","\n","    # 1. åŸºç¤éæ¿¾\n","    aspect_map = {label: i for i, label in enumerate(USER_CONFIG['labels']['aspects'])}\n","    df_clean = df[df['aspect'].isin(USER_CONFIG['labels']['aspects'])].copy()\n","\n","    print(\"æ­£åœ¨åŠ å…¥åè«·æ¨™è¨˜...\")\n","\n","    def inject_tag(row):\n","        # å¦‚æœæ˜¯åè«·ï¼Œå°±åœ¨å¥å­æœ€å‰é¢åŠ ä¸Š [Sarcasm]\n","        if row['sarcasm'] == 'Yes':\n","            return f\"[Sarcasm] {row['text']}\"\n","        else:\n","            return row['text'] # ä¸æ˜¯åè«·å°±ä¿æŒåŸæ¨£\n","\n","    # è¦†è“‹åŸæœ¬çš„ text æ¬„ä½\n","    df_clean['text'] = df_clean.apply(inject_tag, axis=1)\n","\n","    # æª¢æŸ¥ä¸€ä¸‹æœ‰æ²’æœ‰æˆåŠŸ\n","    print(f\"ç¯„ä¾‹ (å‰3ç­†åè«·è³‡æ–™):\\n{df_clean[df_clean['sarcasm']=='Yes'][['text', 'aspect']].head(3)}\")\n","    # ==========================================\n","\n","    # 3. æ¨™ç±¤ç·¨ç¢¼ (è·Ÿä¹‹å‰ä¸€æ¨£)\n","    df_clean['label'] = df_clean['aspect'].map(aspect_map)\n","\n","    # 4. åˆ‡åˆ† (è·Ÿä¹‹å‰ä¸€æ¨£)\n","    train_df, eval_df = train_test_split(df_clean, test_size=0.2, random_state=42)\n","\n","    # 5. è½‰ç‚º HF Dataset\n","    train_ds = Dataset.from_pandas(train_df)\n","    eval_ds = Dataset.from_pandas(eval_df)\n","\n","    return train_ds, eval_ds, aspect_map\n","\n","\n","def train_efficient_setfit(full_train_dataset, eval_dataset, label_mapping):\n","\n","    # 1. è½‰å› Pandas åšç²¾æº–æŠ½æ¨£\n","    df_train = full_train_dataset.to_pandas()\n","\n","    SAMPLE_QUOTAS = {\n","        0: 100,  # Acting: çµ¦ 100 ç­† (åŸæœ¬ 40 ä¸å¤ )\n","        1: 180,  # Plot: çµ¦ 150 ç­† (é€™æ˜¯æœ€é›£çš„ï¼Œçµ¦æœ€å¤š)\n","        2: 50,   # Visuals: 50 ç­†å°±å¤ å¼·äº†\n","        3: 50    # Pacing: 50 ç­†å°±å¤ å¼·äº†\n","    }\n","\n","    def dynamic_sample(group):\n","        label_id = group.name # å–å¾—ç›®å‰çš„ label ID\n","        # å–å¾—è©²é¡åˆ¥çš„é…é¡ï¼Œå¦‚æœæ²’è¨­å®šå°±é è¨­ 50\n","        quota = SAMPLE_QUOTAS.get(label_id, 50)\n","        # å¯¦éš›æŠ½æ¨£ï¼šå– (é…é¡) èˆ‡ (è©²é¡åˆ¥ç¸½æ•¸) çš„æœ€å°å€¼\n","        return group.sample(min(len(group), quota), random_state=42)\n","\n","\n","    # åŸ·è¡ŒæŠ½æ¨£\n","    df_sampled = df_train.groupby('label', group_keys=False).apply(dynamic_sample)\n","\n","    # å¼·åˆ¶é‡ç½® Index\n","    df_sampled = df_sampled.reset_index(drop=True)\n","\n","    # è½‰å› Dataset\n","    train_subset = Dataset.from_pandas(df_sampled, preserve_index=False)\n","\n","    print(f\"åŸæœ¬è¨“ç·´é›†: {len(full_train_dataset)} -> ğŸ“‰ å¹³è¡¡æŠ½æ¨£å¾Œ: {len(train_subset)}\")\n","    print(f\"æŠ½æ¨£å¾Œçš„é¡åˆ¥åˆ†å¸ƒ:\\n{df_sampled['label'].value_counts()}\")\n","\n","    # 2. è¼‰å…¥æ¨¡å‹\n","    model_id = \"sentence-transformers/paraphrase-mpnet-base-v2\"\n","    model = SetFitModel.from_pretrained(\n","        model_id,\n","        id2label={v: k for k, v in label_mapping.items()},\n","        label2id=label_mapping,\n","    )\n","\n","    # 3. è¨­å®šè¨“ç·´åƒæ•¸\n","    trainer = SetFitTrainer(\n","        model=model,\n","        train_dataset=train_subset, # é¤µå…¥æŠ½æ¨£å¾Œçš„è³‡æ–™\n","        eval_dataset=eval_dataset,\n","        loss_class=CosineSimilarityLoss,\n","        metric=\"accuracy\",\n","\n","        # --- æ•ˆç‡åƒæ•¸å€ ---\n","        batch_size=8,\n","        num_iterations=12,\n","        num_epochs=1,\n","        column_mapping={\"text\": \"text\", \"label\": \"label\"}\n","    )\n","\n","    # 4. é–‹å§‹è¨“ç·´\n","    print(\"ğŸš€ é–‹å§‹é«˜æ•ˆç‡è¨“ç·´ (Efficient Training)...\")\n","    trainer.train()\n","\n","    # 5. è©•ä¼°\n","    metrics = trainer.evaluate()\n","    print(f\"\\nğŸ“Š è¨“ç·´å®Œæˆã€‚Metrics: {metrics}\")\n","\n","    return model\n","\n","\n","# ==========================================\n","# åŸ·è¡Œå€å¡Š\n","# ==========================================\n","\n","if 'labeled_df' in locals():\n","    print(\"ğŸš€ é–‹å§‹è¨“ç·´ Model B (Sarcasm Enhanced)...\")\n","\n","    # 1. æº–å‚™ã€ŒåŠ å·¥éã€çš„è³‡æ–™\n","    train_ds_b, eval_ds_b, label_map_b = prepare_dataset_for_model_b(labeled_df)\n","\n","    # 2. è¨“ç·´æ¨¡å‹\n","    model_b = train_efficient_setfit(train_ds_b, eval_ds_b, label_map_b)\n","\n","    # 3. å¦å¤–å­˜æª”\n","    # é›²ç«¯\n","    save_path_b = os.path.join(DRIVE_SAVE_PATH, \"IMDB_aspect_model_enhanced\")\n","    model_b.save_pretrained(save_path_b)\n","\n","    print(f\"âœ… Model B (åè«·å¢å¼·ç‰ˆ) å·²å„²å­˜è‡³: {save_path_b}\")\n","\n","else:\n","    print(\"âŒ æ‰¾ä¸åˆ° labeled_dfï¼Œè«‹å…ˆè®€å–è³‡æ–™ã€‚\")"],"metadata":{"id":"7wUSNMmDTgti"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. SetFitæƒ…æ„Ÿåˆ†é¡(æ­£è² è©•)æ¨¡å‹è¨“ç·´"],"metadata":{"id":"NFUuFYkrV6YG"}},{"cell_type":"code","source":["from setfit import SetFitModel, SetFitTrainer\n","from datasets import Dataset\n","from sentence_transformers.losses import CosineSimilarityLoss\n","import pandas as pd\n","import os\n","import gc\n","import torch\n","\n","# è¨­å®šå­˜æª”è·¯å¾‘\n","OUTPUT_DIR = DRIVE_SAVE_PATH\n","\n","# ==========================================\n","# 0. è®€å–èˆ‡æª¢æŸ¥è³‡æ–™\n","# ==========================================\n","csv_file = os.path.join(DRIVE_PATH, \"qwen_labeled_data.csv\")\n","\n","if os.path.exists(csv_file):\n","    labeled_df = pd.read_csv(csv_file)\n","    print(f\"âœ… æˆåŠŸè¼‰å…¥è³‡æ–™: {len(labeled_df)} ç­†\")\n","    print(f\"   æ¨™ç±¤åˆ†ä½ˆ:\\n{labeled_df['label_original'].value_counts()}\")\n","else:\n","    raise ValueError(f\"âŒ æ‰¾ä¸åˆ° {csv_file}ï¼Œè«‹ç¢ºèªæª”æ¡ˆæ˜¯å¦å·²ä¸Šå‚³Colabã€‚\")\n","\n","# ==========================================\n","# 1. è³‡æ–™æº–å‚™å‡½å¼ï¼šæƒ…æ„Ÿåˆ†æå°ˆç”¨\n","# ==========================================\n","def prepare_sentiment_dataset(df, use_sarcasm=False):\n","    print(f\"\\nğŸ“¦ æ­£åœ¨æº–å‚™æƒ…æ„Ÿåˆ†æè³‡æ–™é›† (Sarcasm Mode: {use_sarcasm})...\")\n","\n","    # è¤‡è£½è³‡æ–™\n","    df_clean = df.copy()\n","\n","    # A. è¨­å®šè¨“ç·´ç›®æ¨™ (ç›´æ¥ä½¿ç”¨ label_original)\n","    df_clean['label'] = df_clean['label_original']\n","\n","    # B. æ³¨å…¥åè«·æ¨™ç±¤ (å¦‚æœæ˜¯ Model B)\n","    if use_sarcasm:\n","        print(\"   âœ¨ æ³¨å…¥ [Sarcasm] ç‰¹å¾µä¸­...\")\n","        def inject_tag(row):\n","            # æ³¨æ„ï¼šé€™è£¡å°æ‡‰ csv ä¸­çš„ 'Yes'\n","            if row['sarcasm'] == 'Yes':\n","                return f\"[Sarcasm] {row['text']}\"\n","            else:\n","                return row['text']\n","        df_clean['text'] = df_clean.apply(inject_tag, axis=1)\n","\n","    # C. æŠ½æ¨£ç­–ç•¥\n","    # è³‡æ–™é‡ç´„ 800 ç­†ï¼ŒSetFit è·‘å…¨é‡ä¹Ÿéå¸¸å¿«ï¼Œé€™è£¡æˆ‘å€‘è¨­å®šä¸Šé™ç‚º 300 ç­†/é¡ (ä¿ç•™ä¸€é»æ¸¬è©¦ç©ºé–“)\n","    SAMPLES_PER_CLASS = 300\n","\n","    print(f\"   ğŸ“‰ åŸ·è¡Œå¹³è¡¡æŠ½æ¨£ï¼šæ¯é¡ä¸Šé™ {SAMPLES_PER_CLASS} ç­†...\")\n","    df_sampled = df_clean.groupby('label', group_keys=False).apply(\n","        lambda x: x.sample(min(len(x), SAMPLES_PER_CLASS), random_state=42)\n","    )\n","    df_sampled = df_sampled.reset_index(drop=True)\n","\n","    # D. è½‰ç‚º HF Dataset\n","    train_ds = Dataset.from_pandas(df_sampled, preserve_index=False)\n","\n","    print(f\"   âœ… æº–å‚™å®Œæˆï¼è¨“ç·´é›†å¤§å°: {len(train_ds)}\")\n","    return train_ds\n","\n","# ==========================================\n","# 2. è¨“ç·´å‡½å¼\n","# ==========================================\n","def train_sentiment_model(train_ds, model_name_suffix):\n","\n","    model_id = \"sentence-transformers/paraphrase-mpnet-base-v2\"\n","\n","    print(f\"\\nğŸš€ é–‹å§‹è¨“ç·´ Sentiment Model ({model_name_suffix})...\")\n","\n","    # è¼‰å…¥æ¨¡å‹\n","    model = SetFitModel.from_pretrained(model_id)\n","\n","    # è¨“ç·´è¨­å®š\n","    trainer = SetFitTrainer(\n","        model=model,\n","        train_dataset=train_ds,\n","        loss_class=CosineSimilarityLoss,\n","        metric=\"accuracy\",\n","        batch_size=8,\n","        num_iterations=5,\n","        num_epochs=1,\n","        column_mapping={\"text\": \"text\", \"label\": \"label\"}\n","    )\n","\n","    trainer.train()\n","    print(f\"âœ… {model_name_suffix} è¨“ç·´å®Œæˆï¼\")\n","\n","    return model\n","\n","# ==========================================\n","# 3. åŸ·è¡Œè¨“ç·´å€å¡Š\n","# ==========================================\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","# --- è¨“ç·´ Model A (Baseline: ç„¡åè«·) ---\n","train_ds_a = prepare_sentiment_dataset(labeled_df, use_sarcasm=False)\n","sentiment_model_a = train_sentiment_model(train_ds_a, \"Baseline\")\n","\n","# å­˜æª”\n","save_path_a = os.path.join(OUTPUT_DIR, \"Sentiment_Model_A_Baseline\")\n","sentiment_model_a.save_pretrained(save_path_a)\n","\n","############################################\n","\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","# --- è¨“ç·´ Model B (Enhanced: æœ‰åè«·) ---\n","train_ds_b = prepare_sentiment_dataset(labeled_df, use_sarcasm=True)\n","sentiment_model_b = train_sentiment_model(train_ds_b, \"Sarcasm-Enhanced\")\n","\n","# å­˜æª”\n","save_path_b = os.path.join(OUTPUT_DIR, \"Sentiment_Model_B_Enhanced\")\n","sentiment_model_b.save_pretrained(save_path_b)\n","\n","print(f\"\\nğŸ‰ å…©çµ„æ¨¡å‹çš†å·²è¨“ç·´å®Œæˆï¼è«‹è‡³ {OUTPUT_DIR} æŸ¥çœ‹ã€‚\")"],"metadata":{"id":"zgop1kW9WW-t"},"execution_count":null,"outputs":[]}]}