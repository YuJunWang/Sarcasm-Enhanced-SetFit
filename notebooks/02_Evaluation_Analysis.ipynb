{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP86230pTqX6bBST2YBYnb0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. ç’°å¢ƒå®‰è£"],"metadata":{"id":"HnMyPf03bZnd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQxieqXtbT-O"},"outputs":[],"source":["# 1. å®‰è£ç’°å¢ƒ\n","!pip install -q datasets transformers torch accelerate setfit scikit-learn matplotlib seaborn\n","!pip install hf_xet\n","!pip install openpyxl\n","\n","# 2. æ›è¼‰ Google Drive\n","from google.colab import drive\n","import os\n","import shutil\n","\n","drive.mount('/content/drive')\n","\n","# è¨­å®šä½ çš„æª”æ¡ˆåœ¨ Drive ä¸­çš„è·¯å¾‘\n","DRIVE_PATH = \"/content/drive/MyDrive/Colab Notebooks/Project/Sarcasm-Enhanced-SetFit/imdb_analysis_results\"\n","CSV_PATH = os.path.join(DRIVE_PATH, \"qwen_labeled_data.csv\")\n","\n","# æª¢æŸ¥ä¸€ä¸‹æª”æ¡ˆåœ¨ä¸åœ¨\n","if os.path.exists(DRIVE_PATH):\n","    print(f\"âœ… æˆåŠŸé€£æ¥ Driveï¼Œæª”æ¡ˆåˆ—è¡¨: {os.listdir(DRIVE_PATH)}\")\n","else:\n","    print(\"âŒ æ‰¾ä¸åˆ°è·¯å¾‘ï¼Œè«‹ç¢ºèªä½ çš„è³‡æ–™å¤¾åç¨±æ˜¯å¦æ­£ç¢º\")\n","\n","\n","DRIVE_SAVE_PATH = \"/content/drive/MyDrive/Colab Notebooks/Project/Sarcasm-Enhanced-SetFit/Model_colab\"\n","print(f\"ğŸ“‚ æ¨¡å‹å°‡å„²å­˜æ–¼é›²ç«¯è·¯å¾‘: {DRIVE_SAVE_PATH}\")"]},{"cell_type":"code","source":["# å„é …åŸºæœ¬è¨­å®š\n","USER_CONFIG = {\n","    \"data\": {\n","        \"dataset_name\": \"imdb\",\n","        \"split\": \"train\",\n","        \"seed_count\": 800,  # ç·´ç¿’ç”¨ç¨®å­è³‡æ–™æ•¸é‡\n","        \"random_seed\": 42\n","    },\n","    \"llm\": {\n","        \"model_id\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n","        \"temperature\": 0.1,  # é™ä½éš¨æ©Ÿæ€§ï¼Œä¿è­‰æ¨™è¨»ä¸€è‡´æ€§\n","        \"max_new_tokens\": 100\n","    },\n","    \"labels\": {\n","        \"aspects\": [\"Acting\", \"Plot/Story\", \"Visuals/Effects\", \"Pacing\"],\n","        \"sarcasm\": [\"Yes\", \"No\"]\n","    }\n","}"],"metadata":{"id":"KfgxGiD6br83"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","# ==========================================\n","# è¦–è¦ºåŒ–ï¼šé…¸æ°‘æ¯”ä¾‹çµ±è¨ˆåœ–\n","# ==========================================\n","\n","def plot_sarcasm_statistics(df):\n","    print(\"æ­£åœ¨è¨ˆç®—å„åˆ†é¡çš„é…¸æ°‘æ»²é€ç‡...\")\n","\n","    order_list = df['aspect'].value_counts().index\n","    # order_list = ['Plot/Story', 'Acting', 'Visuals/Effects', 'Pacing']\n","\n","    # 1. è¨ˆç®—æ¯å€‹ Aspect ä¸­ï¼ŒSarcasm Yes/No çš„æ•¸é‡\n","    # ä½¿ç”¨ crosstab äº¤å‰åˆ†æ\n","    sarcasm_counts = pd.crosstab(df['aspect'], df['sarcasm'])\n","    sarcasm_counts = sarcasm_counts.reindex(order_list)\n","\n","    # 2. è½‰æ›æˆæ¯”ä¾‹ (Percentage) ä»¥ä¾¿å…¬å¹³æ¯”è¼ƒ\n","    # (Plot ç¸½æ•¸å¤šï¼Œå…‰çœ‹æ•¸é‡ä¸æº–ï¼Œè¦çœ‹æ¯”ä¾‹)\n","    sarcasm_ratio = sarcasm_counts.div(sarcasm_counts.sum(1), axis=0) * 100\n","\n","    # 3. ç•«åœ–\n","    custom_palette = {\"No\": \"#4A90E2\", \"Yes\": \"#FF6B6B\"}\n","    fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n","\n","    # å·¦åœ–ï¼šçµ•å°æ•¸é‡ (Count)\n","    sns.countplot(data=df, x='aspect', hue='sarcasm',\n","                  palette=custom_palette, ax=ax[0], order=order_list)\n","    ax[0].set_title(\"Absolute Count of Sarcasm per Aspect\", fontsize=14, fontweight='bold')\n","    ax[0].set_xlabel(\"Aspect\", fontsize=12)\n","    ax[0].set_ylabel(\"Number of Reviews\", fontsize=12)\n","    ax[0].legend(title='Sarcasm', title_fontsize='10')\n","\n","    for container in ax[0].containers:\n","      ax[0].bar_label(container, padding=3, fontsize=11, fontweight='bold')\n","\n","    # å³åœ–ï¼šæ»²é€æ¯”ä¾‹ (Percentage)\n","    sarcasm_ratio.plot(kind='bar', stacked=True,\n","                       color=custom_palette, ax=ax[1], alpha=0.9, rot=0)\n","    ax[1].set_title(\"Sarcasm Penetration Rate (%) by Aspect\", fontsize=14, fontweight='bold')\n","    ax[1].set_ylabel(\"Percentage (%)\")\n","    ax[1].legend(title='Sarcasm', loc='upper right')\n","\n","    # åœ¨å³åœ–çš„æŸ±å­ä¸Šæ¨™ç¤ºæ•¸å­—\n","    for c in ax[1].containers:\n","        labels = [f'{v.get_height():.1f}%' if v.get_height() > 0 else '' for v in c]\n","        ax[1].bar_label(c, labels=labels, label_type='center', color='white', fontweight='bold')\n","\n","    sns.despine(left=True)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    filename = \"Sarcasm Analysis Bar.png\"\n","    save_path = os.path.join(OUTPUT_DIR, filename)\n","    plt.savefig(save_path, dpi=300)\n","    print(f\"âœ… åœ–è¡¨å·²å„²å­˜: {save_path}\")\n","\n","# åŸ·è¡Œçµ±è¨ˆ\n","\n","if os.path.exists(CSV_PATH):\n","    labeled_df = pd.read_csv(CSV_PATH)\n","else:\n","    raise ValueError(\"âŒ æ‰¾ä¸åˆ° qwen_labeled_data.csvï¼Œè«‹ç¢ºèªæª”æ¡ˆä½ç½®ã€‚\")\n","\n","if 'labeled_df' in locals():\n","    plot_sarcasm_statistics(labeled_df)"],"metadata":{"id":"FJdybBD-joba"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. è¦–è¦ºåŒ–èˆ‡åˆ†ç¾¤åˆ†æ - aspect_model"],"metadata":{"id":"dqvo7m7Rb5MG"}},{"cell_type":"markdown","source":["## è¼‰å…¥è¨“ç·´å®Œæˆçš„Aã€Bæ¨¡å‹"],"metadata":{"id":"SVfaOnn2ciov"}},{"cell_type":"code","source":["import os\n","from setfit import SetFitModel\n","from google.colab import drive\n","import torch\n","\n","# ==========================================\n","# 1. æ›è¼‰é›²ç«¯ç¡¬ç¢Ÿ\n","# ==========================================\n","# å¦‚æœå·²ç¶“æ›è¼‰éï¼Œé€™è¡Œæœƒè‡ªå‹•ç•¥éï¼›å¦‚æœé‚„æ²’ï¼Œæœƒè·³å‡ºæˆæ¬Šè¦–çª—\n","if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","\n","# ==========================================\n","# 2. è¨­å®šæ¨¡å‹è·¯å¾‘\n","# ==========================================\n","BASE_PATH = DRIVE_SAVE_PATH\n","\n","# å®šç¾©å…©å€‹æ¨¡å‹çš„è³‡æ–™å¤¾åç¨± (æ ¹æ“šæ‚¨çš„æŒ‡ç¤º)\n","MODEL_A_NAME = \"IMDB_aspect_model\"          # Baseline (åŸå§‹æ¨¡å‹)\n","MODEL_B_NAME = \"IMDB_aspect_model_enhanced\" # Enhanced (åè«·å¢å¼·æ¨¡å‹)\n","\n","path_a = os.path.join(BASE_PATH, MODEL_A_NAME)\n","path_b = os.path.join(BASE_PATH, MODEL_B_NAME)\n","\n","# æª¢æŸ¥ä¸€ä¸‹è·¯å¾‘æ˜¯å¦å­˜åœ¨ï¼Œé¿å…è®€å–å¤±æ•—\n","print(f\"ğŸ“‚ æª¢æŸ¥è·¯å¾‘: {BASE_PATH}\")\n","if os.path.exists(BASE_PATH):\n","    print(\"   ç›®éŒ„å…§å®¹:\", os.listdir(BASE_PATH))\n","else:\n","    print(\"âŒ æ‰¾ä¸åˆ° Base Pathï¼Œè«‹ç¢ºèªè·¯å¾‘æ˜¯å¦æ­£ç¢ºï¼\")\n","\n","# ==========================================\n","# 3. è¼‰å…¥æ¨¡å‹\n","# ==========================================\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f\"\\nğŸš€ ä½¿ç”¨è£ç½®: {device}\")\n","\n","try:\n","    # è¼‰å…¥ Model A\n","    if os.path.exists(path_a):\n","        print(f\"   æ­£åœ¨è¼‰å…¥ Model A: {MODEL_A_NAME}...\")\n","        model_a = SetFitModel.from_pretrained(path_a).to(device)\n","        print(\"   âœ… Model A è¼‰å…¥æˆåŠŸï¼\")\n","    else:\n","        print(f\"   âŒ æ‰¾ä¸åˆ° Model A è³‡æ–™å¤¾: {path_a}\")\n","\n","    # è¼‰å…¥ Model B\n","    if os.path.exists(path_b):\n","        print(f\"   æ­£åœ¨è¼‰å…¥ Model B: {MODEL_B_NAME}...\")\n","        model_b = SetFitModel.from_pretrained(path_b).to(device)\n","        print(\"   âœ… Model B è¼‰å…¥æˆåŠŸï¼\")\n","    else:\n","        print(f\"   âŒ æ‰¾ä¸åˆ° Model B è³‡æ–™å¤¾: {path_b}\")\n","\n","except Exception as e:\n","    print(f\"\\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}\")"],"metadata":{"id":"D8zFNarMcBrf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ç¹ªè£½æ··æ·†çŸ©é™£"],"metadata":{"id":"Hiy_KOgfcvNn"}},{"cell_type":"code","source":["def plot_beautiful_confusion_matrix(cm, labels, title, figsize=(10, 8), cmap='YlGnBu', show_percent=True):\n","    \"\"\"\n","    ç¹ªè£½ç¾è§€ã€æ˜“è®€çš„æ··æ·†çŸ©é™£ (ä½¿ç”¨ Seaborn)\n","\n","    Args:\n","        cm (numpy array): æ··æ·†çŸ©é™£æ•¸å€¼\n","        labels (list): æ¨™ç±¤åç¨±åˆ—è¡¨\n","        title (str): åœ–è¡¨æ¨™é¡Œ\n","        figsize (tuple): åœ–ç‰‡å¤§å°\n","        cmap (str): Seaborn/Matplotlib è‰²ç¥¨åç¨± (æ¨è–¦: 'YlGnBu', 'viridis', 'Blues', 'rocket_r')\n","        show_percent (bool): æ˜¯å¦åœ¨æ ¼å­ä¸­é¡¯ç¤ºç™¾åˆ†æ¯” (Row-wise)\n","    \"\"\"\n","\n","    # 1. è¨ˆç®—ç™¾åˆ†æ¯”çŸ©é™£ (Row-wise: æ¯ä¸€åˆ—ç¸½å’Œç‚º 100%)\n","    # åŠ ä¸Š 1e-10 æ˜¯ç‚ºäº†é¿å…é™¤ä»¥é›¶çš„éŒ¯èª¤\n","    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n","\n","    # 2. æº–å‚™æ ¼å­è£¡çš„è¨»é‡‹æ–‡å­— (Annot)\n","    if show_percent:\n","        annot_labels = [\n","            [f\"{value}\\n({percent:.1f}%)\" for value, percent in zip(row_cm, row_perc)]\n","            for row_cm, row_perc in zip(cm, cm_percent)\n","        ]\n","    else:\n","        annot_labels = cm # åªé¡¯ç¤ºæ•¸å­—\n","\n","    # 3. é–‹å§‹ç¹ªåœ–\n","    fig, ax = plt.subplots(figsize=figsize)\n","\n","    sns.heatmap(cm, annot=annot_labels, fmt='', cmap=cmap, ax=ax,\n","                xticklabels=labels, yticklabels=labels,\n","                linewidths=1, linecolor='lightgray',\n","                annot_kws={\"size\": 12, \"weight\": \"bold\"}, # è¨­å®šæ ¼å­å…§æ–‡å­—å¤§å°å’Œç²—ç´°\n","                cbar_kws={'shrink': .8} # ç¨å¾®ç¸®å° colorbar\n","               )\n","\n","    # 4. ç¾åŒ–æ¨™é¡Œèˆ‡åº§æ¨™è»¸\n","    ax.set_title(title, fontsize=18, fontweight='bold', pad=20)\n","    ax.set_xlabel('Predicted Label', fontsize=14, labelpad=10)\n","    ax.set_ylabel('True Label', fontsize=14, labelpad=10)\n","\n","    # èª¿æ•´åº§æ¨™è»¸åˆ»åº¦æ–‡å­—çš„å¤§å°\n","    plt.xticks(fontsize=11, rotation=0) # å¦‚æœæ¨™ç±¤å¤ªé•·å¯ä»¥æ”¹ rotation=45\n","    plt.yticks(fontsize=11, rotation=0)\n","\n","    filename = title.replace(\":\", \"-\") + \".png\"\n","    save_path = os.path.join(DRIVE_SAVE, filename)\n","    plt.savefig(save_path, dpi=300)\n","    print(f\"âœ… åœ–è¡¨å·²å„²å­˜: {save_path}\")\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return fig\n","\n","\n","\n","def run_visualization_analysis(model, dataset, label_map, model_name):\n","    print(f\"Generating predictions and embeddings with {model_name}...\")\n","\n","    # 1. å–å¾—é æ¸¬çµæœèˆ‡ Embeddings\n","    # SetFit çš„ model_body å°±æ˜¯åº•å±¤çš„ Sentence Transformer\n","    embeddings = model.model_body.encode(dataset['text'])\n","    predictions = model.predict(dataset['text'])\n","\n","    # å°‡ label id è½‰å›æ–‡å­— (0 -> Acting)\n","    id2label = {v: k for k, v in label_map.items()}\n","    true_labels = [id2label[x] for x in dataset['label']]\n","    pred_labels = [id2label[x] for x in predictions.tolist()]\n","\n","    # 2. æ··æ·†çŸ©é™£ (Confusion Matrix)\n","    print(\"\\n--- Model Performance ---\")\n","    cm = confusion_matrix(true_labels, pred_labels, labels=list(label_map.keys()))\n","\n","    plot_beautiful_confusion_matrix(\n","        cm,\n","        label_map.keys(),\n","        title=f'Confusion Matrix : True vs Predicted ({model_name})',\n","        figsize=(12, 8),\n","        cmap='Blues',\n","        show_percent=True\n","        )\n","\n","    # 3. t-SNE é™ç¶­è¦–è¦ºåŒ– (Embeddings Space)\n","    print(\"\\n--- Embedding Space Visualization (t-SNE) ---\")\n","    # åƒæ•¸è¨­å®šï¼šperplexity éœ€å°æ–¼æ¨£æœ¬æ•¸ï¼Œé€šå¸¸è¨­ 30ï¼Œä½†æˆ‘å€‘è³‡æ–™å°‘ï¼Œè¨­ 10-20 è¼ƒç©©\n","    tsne = TSNE(n_components=2, random_state=42, perplexity=10, init='pca', learning_rate='auto')\n","    vis_dims = tsne.fit_transform(embeddings)\n","\n","    vis_df = pd.DataFrame({\n","        'x': vis_dims[:, 0],\n","        'y': vis_dims[:, 1],\n","        'Aspect': true_labels,\n","        'Predicted': pred_labels\n","    })\n","\n","    # ç¹ªè£½çœŸå¯¦æ¨™ç±¤åˆ†ä½ˆ\n","    plt.figure(figsize=(12, 8))\n","    sns.scatterplot(data=vis_df, x='x', y='y', hue='Aspect', style='Aspect', s=100, palette='viridis')\n","    plt.title('t-SNE Visualization of Movie Reviews (Colored by Aspect)')\n","    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # 4. K-Means éç›£ç£åˆ†ç¾¤ (Unsupervised Clustering)\n","    # å¼·è¿«æ©Ÿå™¨æŠŠè³‡æ–™åˆ†æˆ 4 å †ï¼Œçœ‹å®ƒæ€éº¼åˆ†\n","    print(\"\\n--- K-Means Clustering Analysis ---\")\n","    kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n","    cluster_labels = kmeans.fit_predict(embeddings)\n","\n","    plt.figure(figsize=(12, 8))\n","    sns.scatterplot(data=vis_df, x='x', y='y', hue=cluster_labels, palette='tab10', s=100, legend='full')\n","    plt.title('K-Means Clustering Results (Machine\\'s own logic)')\n","    plt.show()\n","\n","\n","\n","def run_sarcasm_visualization(model_b, dataset, label_map, model_name=\"Model B (Enhanced)\"):\n","    print(\"   (ç›´æ¥ä½¿ç”¨ Ground Truth åè«·æ¨™ç±¤æ³¨å…¥ç‰¹å¾µ)\")\n","\n","    # 1. è½‰æˆ Pandas æ–¹ä¾¿è™•ç†\n","    # æ³¨æ„ï¼šé€™è£¡å‡è¨­ dataset æœ‰ 'sarcasm' é€™å€‹æ¬„ä½\n","    # å¦‚æœ dataset æ˜¯å¾ labeled_df è½‰ä¾†çš„ï¼Œæ‡‰è©²æœƒæœ‰ã€‚å¦‚æœæ²’æœ‰ï¼Œæˆ‘å€‘è¦å¾åŸå§‹ dataframe æ’ˆã€‚\n","    if isinstance(dataset, Dataset):\n","        df_temp = dataset.to_pandas()\n","    else:\n","        df_temp = dataset.copy()\n","\n","    # æª¢æŸ¥æœ‰æ²’æœ‰ sarcasm æ¬„ä½\n","    if 'sarcasm' not in df_temp.columns:\n","        print(\"âŒ éŒ¯èª¤ï¼šè³‡æ–™é›†ä¸­æ‰¾ä¸åˆ° 'sarcasm' æ¬„ä½ï¼Œç„¡æ³•é€²è¡Œ sarcasm æ¸¬è©¦ã€‚\")\n","        return\n","\n","    # 2. åŠ å·¥æ–‡å­— (æ³¨å…¥æ¨™ç±¤)\n","    print(\"   Step 1: æ ¹æ“šçœŸå¯¦æ¨™ç±¤æ³¨å…¥ [Sarcasm]...\")\n","\n","    def inject_ground_truth(row):\n","        if row['sarcasm'] == 'Yes':\n","            return f\"[Sarcasm] {row['text']}\"\n","        else:\n","            return row['text']\n","\n","    df_temp['text'] = df_temp.apply(inject_ground_truth, axis=1)\n","\n","    # çµ±è¨ˆä¸€ä¸‹\n","    sarcasm_count = len(df_temp[df_temp['text'].str.startswith(\"[Sarcasm]\")])\n","    print(f\"   (å…± {sarcasm_count} ç­†è³‡æ–™è¢«å¼·åˆ¶æ¨™è¨˜ç‚ºåè«·)\")\n","\n","    # 3. è½‰å› Dataset\n","    dataset_for_model_b = Dataset.from_pandas(df_temp)\n","\n","    # 4. å‘¼å«åŸæœ¬çš„ç¹ªåœ–å‡½å¼\n","    print(f\"   Step 2: é–‹å§‹ç¹ªåœ–...\")\n","    run_visualization_analysis(model_b, dataset_for_model_b, label_map, model_name)"],"metadata":{"id":"jOUUIdf_cxIg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_mapping = {\n","    'Acting': 0,\n","    'Plot/Story': 1,\n","    'Visuals/Effects': 2,\n","    'Pacing': 3\n","}\n","\n","# 1. æº–å‚™åŒ…å« 'sarcasm' æ¬„ä½çš„å®Œæ•´è³‡æ–™é›†\n","# æˆ‘å€‘ç›´æ¥ç”¨ labeled_df ä¾†åšè½‰æ›ï¼Œç¢ºä¿æ¬„ä½éƒ½åœ¨\n","full_dataset_df = labeled_df[labeled_df['aspect'].isin(USER_CONFIG['labels']['aspects'])].copy()\n","full_dataset_df['label'] = full_dataset_df['aspect'].map(label_mapping)\n","full_ds_for_vis = Dataset.from_pandas(full_dataset_df)\n","\n","# 2. åŸ·è¡Œ Model A (å°ç…§çµ„ - åŸå§‹)\n","print(\"\\nğŸ”µ [Model A] åŸå§‹æ¨¡å‹åˆ†æ...\")\n","aspect_model = model_a\n","run_visualization_analysis(aspect_model, full_ds_for_vis, label_mapping, \"Model A (Baseline)\")\n","\n","# 3. åŸ·è¡Œ Model B (å¯¦é©—çµ„ - åè«·æ¨™è¨˜)\n","print(\"\\nğŸ”´ [Model B] åŠ å…¥åè«·æ¨™è¨˜æ¨¡å‹...\")\n","aspect_model_enhanced = model_b\n","run_sarcasm_visualization(target_model, full_ds_for_vis, label_mapping, \"Model B (Enhanced)\")"],"metadata":{"id":"BvefH5AIdVzE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. è¦–è¦ºåŒ–èˆ‡åˆ†ç¾¤åˆ†æ - sentiment_model"],"metadata":{"id":"MSwwm5t-hiOJ"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from setfit import SetFitModel\n","import os\n","import torch\n","from google.colab import drive\n","\n","# ==========================================\n","# 1. è¨­å®šè·¯å¾‘èˆ‡æ›è¼‰\n","# ==========================================\n","if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","\n","# è¨­å®šå°ˆæ¡ˆè·¯å¾‘ (è«‹ç¢ºèªé€™æ˜¯å­˜æ”¾æ¨¡å‹çš„åœ°æ–¹)\n","OUTPUT_DIR = DRIVE_SAVE_PATH\n","\n","# æ¨¡å‹è·¯å¾‘\n","PATH_A = os.path.join(OUTPUT_DIR, \"Sentiment_Model_A_Baseline\")\n","PATH_B = os.path.join(OUTPUT_DIR, \"Sentiment_Model_B_Enhanced\")\n","\n","# ==========================================\n","# 2. æŠ“å‡ºã€Œæœªç¶“è¨“ç·´ã€çš„ 200 ç­†æ¸¬è©¦è³‡æ–™\n","# ==========================================\n","print(\"ğŸ“¦ æ­£åœ¨æº–å‚™æ¸¬è©¦è³‡æ–™ (Test Set Isolation)...\")\n","\n","if os.path.exists(CSV_PATH):\n","    df = pd.read_csv(CSV_PATH)\n","else:\n","    backup_csv = os.path.join(OUTPUT_DIR, \"qwen_labeled_data.csv\")\n","    if os.path.exists(backup_csv):\n","        df = pd.read_csv(backup_csv)\n","    else:\n","        raise ValueError(\"âŒ æ‰¾ä¸åˆ° qwen_labeled_data.csvï¼Œè«‹ç¢ºèªæª”æ¡ˆä½ç½®ã€‚\")\n","\n","# A. é‡ç¾è¨“ç·´æ™‚çš„ç¯©é¸èˆ‡æŠ½æ¨£é‚è¼¯\n","# æŠ½å‡ºè¨“ç·´ä½¿ç”¨çš„å…©ç¨®è³‡æ–™å„300ç­†ï¼Œç¸½å…±æ’é™¤600ç­†ï¼Œå‰©ä¸‹çš„200ç­†ç”¨æ–¼æ¸¬è©¦\n","df_clean = df[df['label_original'].isin([0, 1])].copy()\n","\n","# è¨“ç·´æ™‚æ¯é¡å– 300 ç­†ï¼Œrandom_state=42\n","SAMPLES_PER_CLASS = 300\n","train_df_indices = df_clean.groupby('label_original', group_keys=False).apply(\n","    lambda x: x.sample(min(len(x), SAMPLES_PER_CLASS), random_state=42)\n",").index\n","\n","print(f\"   - åŸå§‹æœ‰æ•ˆè³‡æ–™: {len(df_clean)} ç­†\")\n","print(f\"   - è¨“ç·´é›†ä½¿ç”¨: {len(train_df_indices)} ç­†\")\n","\n","# B. ä½¿ç”¨ Drop æ’é™¤è¨“ç·´è³‡æ–™ -> å‰©ä¸‹çš„å°±æ˜¯æ¸¬è©¦é›†\n","test_df = df_clean.drop(train_df_indices).copy()\n","\n","print(f\"   - æ¸¬è©¦é›† (æœªè¦‹é): {len(test_df)} ç­†\")\n","print(f\"   - æ¸¬è©¦é›†åè«·å¥åˆ†ä½ˆ: {test_df['sarcasm'].value_counts().to_dict()}\")\n","\n","# ==========================================\n","# 3. è¼‰å…¥æ¨¡å‹èˆ‡åŸ·è¡Œé›™æ¨¡é æ¸¬\n","# ==========================================\n","print(\"\\nğŸš€ æ­£åœ¨è¼‰å…¥æ¨¡å‹ä¸¦é€²è¡Œæ¨è«–...\")\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f\"   Using device: {device}\")\n","\n","try:\n","    # Model A (Baseline)\n","    print(f\"   Loading Model A from: {os.path.basename(PATH_A)}\")\n","    model_a = SetFitModel.from_pretrained(PATH_A).to(device)\n","    preds_a = model_a.predict(test_df['text'].tolist())\n","\n","    # Model B (Enhanced)\n","    print(f\"   Loading Model B from: {os.path.basename(PATH_B)}\")\n","    model_b = SetFitModel.from_pretrained(PATH_B).to(device)\n","\n","    # ç‚º Model B æ³¨å…¥æ¨™ç±¤\n","    def inject_sarcasm(row):\n","        # å°æ‡‰ CSV ä¸­çš„ 'Yes'\n","        if row['sarcasm'] == 'Yes':\n","            return f\"[Sarcasm] {row['text']}\"\n","        else:\n","            return row['text']\n","\n","    test_df['text_enhanced'] = test_df.apply(inject_sarcasm, axis=1)\n","    preds_b = model_b.predict(test_df['text_enhanced'].tolist())\n","\n","except Exception as e:\n","    print(f\"\\nâŒ æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥è·¯å¾‘: {e}\")\n","    # åœæ­¢åŸ·è¡Œå¾ŒçºŒç¹ªåœ–\n","    raise e\n","\n","# çœŸå¯¦æ¨™ç±¤\n","y_true = test_df['label_original'].tolist()\n","sarcasm_flags = test_df['sarcasm'].tolist()\n","\n","# ==========================================\n","# 4. ç¹ªè£½å®Œæ•´è©•ä¼°åœ–è¡¨\n","# ==========================================\n","print(\"\\nğŸ“Š ç¹ªè£½åˆ†æåœ–è¡¨...\")\n","\n","def plot_full_analysis(y_true, preds_a, preds_b, sarcasm_flags):\n","    fig, axes = plt.subplots(1, 3, figsize=(22, 6))\n","\n","    # è¨­å®šå­—å‹å¤§å°\n","    plt.rcParams.update({'font.size': 12})\n","\n","    # --- åœ– 1: Model A æ··æ·†çŸ©é™£ ---\n","    cm_a = confusion_matrix(y_true, preds_a)\n","    sns.heatmap(cm_a, annot=True, fmt='d', cmap='Blues', ax=axes[0], cbar=False, annot_kws={\"size\": 14})\n","    acc_a = accuracy_score(y_true, preds_a)\n","    axes[0].set_title(f\"Model A (Baseline)\\nOverall Acc: {acc_a:.1%}\", fontweight='bold')\n","    axes[0].set_xlabel(\"Predicted (0=Neg, 1=Pos)\")\n","    axes[0].set_ylabel(\"True Label\")\n","\n","    # --- åœ– 2: Model B æ··æ·†çŸ©é™£ ---\n","    cm_b = confusion_matrix(y_true, preds_b)\n","    sns.heatmap(cm_b, annot=True, fmt='d', cmap='Greens', ax=axes[1], cbar=False, annot_kws={\"size\": 14})\n","    acc_b = accuracy_score(y_true, preds_b)\n","    axes[1].set_title(f\"Model B (Enhanced)\\nOverall Acc: {acc_b:.1%}\", fontweight='bold')\n","    axes[1].set_xlabel(\"Predicted (0=Neg, 1=Pos)\")\n","    axes[1].set_yticks([]) # éš±è— Y è»¸\n","\n","    # --- åœ– 3: é—œéµæˆ°å ´ - åè«· vs ä¸€èˆ¬ (Gap Analysis) ---\n","    mask_sarcasm = np.array(sarcasm_flags) == 'Yes'\n","    mask_normal = np.array(sarcasm_flags) == 'No'\n","\n","    # è¨ˆç®—åˆ†çµ„æº–ç¢ºç‡\n","    scores = {\n","        'Normal\\n(Model A)': accuracy_score(np.array(y_true)[mask_normal], np.array(preds_a)[mask_normal]),\n","        'Normal\\n(Model B)': accuracy_score(np.array(y_true)[mask_normal], np.array(preds_b)[mask_normal]),\n","        'Sarcasm\\n(Model A)': accuracy_score(np.array(y_true)[mask_sarcasm], np.array(preds_a)[mask_sarcasm]),\n","        'Sarcasm\\n(Model B)': accuracy_score(np.array(y_true)[mask_sarcasm], np.array(preds_b)[mask_sarcasm]),\n","    }\n","\n","    names = list(scores.keys())\n","    values = list(scores.values())\n","    colors = ['#aec7e8', '#98df8a', '#1f77b4', '#2ca02c'] # æ·ºè—, æ·ºç¶ , æ·±è—, æ·±ç¶ \n","\n","    bars = axes[2].bar(names, values, color=colors, edgecolor='black', alpha=0.8)\n","    axes[2].set_title(\"Performance Gap: Normal vs Sarcasm\", fontweight='bold')\n","    axes[2].set_ylim(0, 1.15)\n","\n","    # æ¨™ç¤ºæ•¸å€¼\n","    for bar in bars:\n","        height = bar.get_height()\n","        axes[2].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n","                f'{height:.1%}', ha='center', va='bottom', fontweight='bold')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_full_analysis(y_true, preds_a, preds_b, sarcasm_flags)\n","\n","# ==========================================\n","# 5. æ•‘æ´æˆåŠŸæ¡ˆä¾‹å±•ç¤º\n","# ==========================================\n","print(\"\\nğŸ•µï¸â€â™€ï¸ [æ•‘æ´æˆåŠŸæ¡ˆä¾‹] Model A èª¤åˆ¤ -> Model B ä¿®æ­£ (é™åè«·å¥)ï¼š\")\n","\n","compare_df = test_df.copy()\n","compare_df['Pred_A'] = preds_a\n","compare_df['Pred_B'] = preds_b\n","\n","# ç¯©é¸æ¢ä»¶ï¼šA éŒ¯, B å°, ä¸”çœŸçš„æ˜¯åè«·\n","rescued = compare_df[\n","    (compare_df['Pred_A'] != compare_df['label_original']) &\n","    (compare_df['Pred_B'] == compare_df['label_original']) &\n","    (compare_df['sarcasm'] == 'Yes')\n","]\n","\n","if not rescued.empty:\n","    # é¡¯ç¤ºå‰ 5 ç­†ï¼ŒåŒ…å«åŸå§‹æ–‡å­—èˆ‡æ³¨å…¥å¾Œçš„æ–‡å­—\n","    display_cols = ['text', 'text_enhanced', 'label_original', 'Pred_A', 'Pred_B']\n","    # ç‚ºäº†ç¾è§€ï¼Œæˆªæ–·éé•·çš„æ–‡å­—\n","    pd.set_option('display.max_colwidth', 100)\n","    display(rescued[display_cols].head(5))\n","    print(f\"\\nå…±ç™¼ç¾ {len(rescued)} ç­†æ•‘æ´æˆåŠŸçš„åè«·æ¡ˆä¾‹ï¼\")\n","else:\n","    print(\"æœ¬æ¬¡æ¸¬è©¦æœªç™¼ç¾é¡¯è‘—çš„åè«·æ•‘æ´æ¡ˆä¾‹ã€‚\")"],"metadata":{"id":"9qXZms4ahtiN"},"execution_count":null,"outputs":[]}]}